**epoch = (iter * batch_size) / data_size**

**transformer 需要花费更多的时间去训练**

[pytorch lightning] (https://www.pytorchlightning.ai/blog)

https://paperswithcode.com/methods/category/object-detection-models

https://jalammar.github.io/illustrated-transformer/

https://github.com/KevinMusgrave/pytorch-metric-learning

https://github.com/microsoft/computervision-recipes/blob/master/scenarios/action_recognition/01_training_introduction.ipynb

https://github.com/Shaoli-Huang/SnapMix

**GCNet**

GCNet 中不同query位置的attention map几乎是相同的，所以在non-local block中通过计算global的attention map以及对于所有query 位置都共享这个attention map
来进行简化。GC block和SE block的不同是SE block通过recale来对校准通道的重要性，而GC block则是通过addition来聚合所有位置的global context来获取各位置间
长距离依赖关系的信息。并且在GC block中引入了layer normalization更好的进行优化。
