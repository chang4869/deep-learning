**epoch = (iter * batch_size) / data_size**

**transformer 需要花费更多的时间去训练**

[pytorch lightning] (https://www.pytorchlightning.ai/blog)

https://paperswithcode.com/methods/category/object-detection-models

https://jalammar.github.io/illustrated-transformer/

https://github.com/KevinMusgrave/pytorch-metric-learning

https://github.com/microsoft/computervision-recipes/blob/master/scenarios/action_recognition/01_training_introduction.ipynb

https://github.com/Shaoli-Huang/SnapMix

**GCNet**

GCNet 中不同query位置的attention map几乎是相同的，所以在non-local block中通过计算global的attention map以及对于所有query 位置都共享这个attention map
来进行简化。GC block和SE block的不同是SE block通过recale来对校准通道的重要性，而GC block则是通过addition来聚合所有位置的global context来获取各位置间
长距离依赖关系的信息。并且在GC block中引入了layer normalization更好的进行优化。

**VFnet**

相比于FCOS+ATSS，vfnet移除了centerness分支， 引入了三个新的组成。分别是varifocal loss ，star-shape的bbox 特征表示, 还有bbox的提炼。
![image](https://github.com/chang4869/deep-learning/blob/gh-pages/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-12-29%20142647.png)

p是预测的基于IOU的分类得分(IACS), q是目标的IOU得分，对于正样本，q为生成的边界框和真实框的IOU，对于负样本，q为0。varifocal loss只减少负样本对于损失的贡献，
没有以相同的方式降低正样本的损失权重，VFnet使用9个固定采样点的特征表示边界框，用的是deformable卷积，这样能获得边界框的几何信息和它邻近的上下文信息。VFnet把bbox的提炼
当成一个残差问题来学习，对于一个初始的回归边界框(l',t',r', b')，首先提取star-shape的表示进行编码，然后学习4个距离缩放系数(\Deltal
